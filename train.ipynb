{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "bB8gHCR3FVC0"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sawo1x8kQk9b"
   },
   "source": [
    "## Imports\n",
    "\n",
    "\n",
    "Let's first get our imports out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0vJOdh9WbTpd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-recommenders\n",
    "!pip install -q --upgrade tensorflow-datasets\n",
    "!pip install -q scann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SZGYDaF-m5wZ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 01:41:26.218939: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BxQ_hy7xPH3N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PAqjR4a1RR4"
   },
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aaQhqcLGP0jL",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 01:41:28.847159: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-18 01:41:28.858842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-18 01:41:28.861927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-18 01:41:28.866322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-18 01:41:28.869287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-18 01:41:28.872065: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-18 01:41:29.870962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-18 01:41:29.873041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-18 01:41:29.874889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-18 01:41:29.876627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13591 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Ratings data.\n",
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "uhbEvPJqxLec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"user_rating\": x[\"user_rating\"]\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"movie_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_titles = movies.batch(1_000)\n",
    "user_ids = ratings.batch(1_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "rS0eDfkjnjJL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "dataset_size = min(len(ratings) // 2, 100_000)\n",
    "train_size = dataset_size * 4 // 5\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "shuffled = ratings.shuffle(dataset_size, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(train_size)\n",
    "test = shuffled.skip(train_size).take(test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZUFeSlWRHGx"
   },
   "source": [
    "### The full model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.get_strategy()\n",
    "embedding_dimension = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "8n7c5CHFp0ow",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.Model):\n",
    "\n",
    "    def __init__(self, rating_weight: float, retrieval_weight: float) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # User and movie models.\n",
    "        self.movie_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
    "          tf.keras.layers.StringLookup(\n",
    "            vocabulary=unique_movie_titles, mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
    "        ])\n",
    "        self.user_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
    "          tf.keras.layers.StringLookup(\n",
    "            vocabulary=unique_user_ids, mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "        ])\n",
    "\n",
    "        # A small model to take in user and movie embeddings and predict ratings.\n",
    "        # We can make this as complicated as we want as long as we output a scalar\n",
    "        # as our prediction.\n",
    "        self.rating_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(1),\n",
    "        ])\n",
    "\n",
    "        # The tasks.\n",
    "        self.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "        )\n",
    "        self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=movies.batch(128).map(self.movie_model)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.rating_weight = rating_weight\n",
    "        self.retrieval_weight = retrieval_weight\n",
    "\n",
    "    def call(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
    "        # We pick out the user features and pass them into the user model.\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        # And pick out the movie features and pass them into the movie model.\n",
    "        movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "\n",
    "        return (\n",
    "            user_embeddings,\n",
    "            movie_embeddings,\n",
    "            # We apply the multi-layered rating model to a concatentation of\n",
    "            # user and movie embeddings.\n",
    "            self.rating_model(\n",
    "                tf.concat([user_embeddings, movie_embeddings], axis=1)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "        ratings = features.pop(\"user_rating\")\n",
    "\n",
    "        user_embeddings, movie_embeddings, rating_predictions = self(features)\n",
    "\n",
    "        # We compute the loss for each task.\n",
    "        rating_loss = self.rating_task(\n",
    "            labels=ratings,\n",
    "            predictions=rating_predictions,\n",
    "        )\n",
    "        retrieval_loss = self.retrieval_task(user_embeddings, movie_embeddings)\n",
    "\n",
    "        return (self.rating_weight * rating_loss\n",
    "                + self.retrieval_weight * retrieval_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDN_LJGlnRGo"
   },
   "source": [
    "## Fitting and evaluating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "aW63YaqP2wCf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = MovielensModel(rating_weight=1.0, retrieval_weight=1.0)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "53QJwY1gUnfv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "ZxPntlT8EFOZ",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 10s 1s/step - root_mean_squared_error: 2.4998 - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 7.5000e-04 - factorized_top_k/top_10_categorical_accuracy: 0.0028 - factorized_top_k/top_50_categorical_accuracy: 0.0312 - factorized_top_k/top_100_categorical_accuracy: 0.0734 - loss: 70612.8815 - regularization_loss: 0.0000e+00 - total_loss: 70612.8815\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 6s 1s/step - root_mean_squared_error: 1.5507 - factorized_top_k/top_1_categorical_accuracy: 1.5000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0049 - factorized_top_k/top_10_categorical_accuracy: 0.0132 - factorized_top_k/top_50_categorical_accuracy: 0.1040 - factorized_top_k/top_100_categorical_accuracy: 0.2140 - loss: 69239.0977 - regularization_loss: 0.0000e+00 - total_loss: 69239.0977\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 6s 1s/step - root_mean_squared_error: 1.4185 - factorized_top_k/top_1_categorical_accuracy: 0.0011 - factorized_top_k/top_5_categorical_accuracy: 0.0203 - factorized_top_k/top_10_categorical_accuracy: 0.0455 - factorized_top_k/top_50_categorical_accuracy: 0.2018 - factorized_top_k/top_100_categorical_accuracy: 0.3412 - loss: 67129.6276 - regularization_loss: 0.0000e+00 - total_loss: 67129.6276\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 6s 1s/step - root_mean_squared_error: 1.3374 - factorized_top_k/top_1_categorical_accuracy: 0.0014 - factorized_top_k/top_5_categorical_accuracy: 0.0324 - factorized_top_k/top_10_categorical_accuracy: 0.0670 - factorized_top_k/top_50_categorical_accuracy: 0.2592 - factorized_top_k/top_100_categorical_accuracy: 0.4058 - loss: 65589.0260 - regularization_loss: 0.0000e+00 - total_loss: 65589.0260\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 6s 1s/step - root_mean_squared_error: 1.2819 - factorized_top_k/top_1_categorical_accuracy: 0.0016 - factorized_top_k/top_5_categorical_accuracy: 0.0417 - factorized_top_k/top_10_categorical_accuracy: 0.0827 - factorized_top_k/top_50_categorical_accuracy: 0.2989 - factorized_top_k/top_100_categorical_accuracy: 0.4500 - loss: 64436.8281 - regularization_loss: 0.0000e+00 - total_loss: 64436.8281\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 6s 1s/step - root_mean_squared_error: 1.1810 - factorized_top_k/top_1_categorical_accuracy: 0.0017 - factorized_top_k/top_5_categorical_accuracy: 0.0470 - factorized_top_k/top_10_categorical_accuracy: 0.0944 - factorized_top_k/top_50_categorical_accuracy: 0.3264 - factorized_top_k/top_100_categorical_accuracy: 0.4768 - loss: 63557.5339 - regularization_loss: 0.0000e+00 - total_loss: 63557.5339\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 6s 1s/step - root_mean_squared_error: 1.2076 - factorized_top_k/top_1_categorical_accuracy: 0.0019 - factorized_top_k/top_5_categorical_accuracy: 0.0518 - factorized_top_k/top_10_categorical_accuracy: 0.1029 - factorized_top_k/top_50_categorical_accuracy: 0.3442 - factorized_top_k/top_100_categorical_accuracy: 0.4932 - loss: 62879.6354 - regularization_loss: 0.0000e+00 - total_loss: 62879.6354\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 6s 1s/step - root_mean_squared_error: 1.1485 - factorized_top_k/top_1_categorical_accuracy: 0.0018 - factorized_top_k/top_5_categorical_accuracy: 0.0557 - factorized_top_k/top_10_categorical_accuracy: 0.1101 - factorized_top_k/top_50_categorical_accuracy: 0.3555 - factorized_top_k/top_100_categorical_accuracy: 0.5049 - loss: 62348.7650 - regularization_loss: 0.0000e+00 - total_loss: 62348.7650\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 6s 1s/step - root_mean_squared_error: 1.1518 - factorized_top_k/top_1_categorical_accuracy: 0.0018 - factorized_top_k/top_5_categorical_accuracy: 0.0588 - factorized_top_k/top_10_categorical_accuracy: 0.1156 - factorized_top_k/top_50_categorical_accuracy: 0.3636 - factorized_top_k/top_100_categorical_accuracy: 0.5137 - loss: 61925.1016 - regularization_loss: 0.0000e+00 - total_loss: 61925.1016\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 6s 1s/step - root_mean_squared_error: 1.1011 - factorized_top_k/top_1_categorical_accuracy: 0.0018 - factorized_top_k/top_5_categorical_accuracy: 0.0606 - factorized_top_k/top_10_categorical_accuracy: 0.1186 - factorized_top_k/top_50_categorical_accuracy: 0.3692 - factorized_top_k/top_100_categorical_accuracy: 0.5198 - loss: 61580.1576 - regularization_loss: 0.0000e+00 - total_loss: 61580.1576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f6b604b69b0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "W-zu6HLODNeI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 4s 573ms/step - root_mean_squared_error: 1.1360 - factorized_top_k/top_1_categorical_accuracy: 2.0000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0021 - factorized_top_k/top_10_categorical_accuracy: 0.0061 - factorized_top_k/top_50_categorical_accuracy: 0.0640 - factorized_top_k/top_100_categorical_accuracy: 0.1482 - loss: 24350.8257 - regularization_loss: 0.0000e+00 - total_loss: 24350.8257\n",
      "Retrieval top-100 accuracy: 0.148.\n",
      "Ranking RMSE: 1.136.\n"
     ]
    }
   ],
   "source": [
    "metrics = model.evaluate(cached_test, return_dict=True)\n",
    "print(f\"Retrieval top-100 accuracy: {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}.\")\n",
    "print(f\"Ranking RMSE: {metrics['root_mean_squared_error']:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ScaNN Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "rTz8yxyp5uwU",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 18:00:21.028236: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 1682\n",
      "2023-10-18 18:00:21.033815: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:84] PartitionerFactory ran in 5.529921ms.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.ScaNN at 0x7f6b603fca30>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scann_index = tfrs.layers.factorized_top_k.ScaNN(model.user_model)\n",
    "scann_index.index_from_dataset(\n",
    "  tf.data.Dataset.zip((movies.batch(100), movies.batch(100).map(model.movie_model)))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save all model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "K7NUqgxU6W_T",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/scann_100k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/scann_100k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    }
   ],
   "source": [
    "# Export the query model.\n",
    "scann_path = \"deploy/models/scann_100k\"\n",
    "\n",
    "scann_index.call(tf.constant([\"user_id\"]), tf.constant(1))\n",
    "scann_index.query_with_exclusions(tf.constant([\"user_id\"]), tf.constant([[\"movie_title\"]]), tf.constant(5))\n",
    "\n",
    "# Save the index.\n",
    "tf.keras.models.save_model(\n",
    "  scann_index,\n",
    "  scann_path,\n",
    "  options=tf.saved_model.SaveOptions(namespace_whitelist=[\"Scann\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/user_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/user_model/assets\n"
     ]
    }
   ],
   "source": [
    "user_model_path = \"deploy/models/user_model\"\n",
    "\n",
    "user_embedding = model.user_model(tf.constant([\"user_id\"]))\n",
    "\n",
    "# Save the index.\n",
    "model.user_model.save(user_model_path, save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/movie_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/movie_model/assets\n"
     ]
    }
   ],
   "source": [
    "movie_model_path = \"deploy/models/movie_model\"\n",
    "\n",
    "movie_embedding = model.movie_model(tf.constant([\"movie_title\"]))\n",
    "\n",
    "# Save the index.\n",
    "model.movie_model.save(movie_model_path, save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/rating_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/rating_model/assets\n"
     ]
    }
   ],
   "source": [
    "rating_model_path = \"deploy/models/rating_model\"\n",
    "\n",
    "model.rating_model(tf.concat([user_embedding, movie_embedding], axis=1))\n",
    "\n",
    "# Save the index.\n",
    "model.rating_model.save(rating_model_path, save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models for testing prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "scann_loaded = tf.keras.models.load_model(scann_path)\n",
    "user_model_loaded = tf.keras.models.load_model(user_model_path)\n",
    "movie_model_loaded = tf.keras.models.load_model(movie_model_path)\n",
    "rating_model_loaded = tf.keras.models.load_model(rating_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom predict to return retrieval results\n",
    "\n",
    "NOTE: returned scores are not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_K = 10\n",
    "\n",
    "def retrieval_predict(user_id, exclusions = [], k = DEFAULT_K):\n",
    "    user_ids = tf.constant([user_id])\n",
    "    k = tf.constant(k)\n",
    "\n",
    "    if len(exclusions) > 0:\n",
    "        exclusions = tf.constant([exclusions])\n",
    "        k_scores, k_predictions = scann_loaded.query_with_exclusions(user_ids, exclusions, k)\n",
    "    else:\n",
    "        k_scores, k_predictions = scann_loaded.call(user_ids, k)\n",
    "\n",
    "    k_predictions_json = k_predictions.numpy().tolist()\n",
    "    k_scores_json = k_scores.numpy().tolist()\n",
    "\n",
    "    k_encoded_predictions_json = [[pred.decode('utf-8') for pred in pred_list] for pred_list in k_predictions_json]\n",
    "\n",
    "    return {\n",
    "        \"movie_titles\": k_encoded_predictions_json[0],\n",
    "        \"movie_scores\": k_scores_json[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.887946367263794"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def check_prediction_time(n):\n",
    "    request = {\n",
    "        \"instances\": [\n",
    "            {\n",
    "                \"user_id\": \"41\",\n",
    "                \"exclusions\": [\"Fargo (1996)\"],\n",
    "            }\n",
    "        ]*n,\n",
    "        \"parameters\": {\n",
    "            \"k\": 10\n",
    "        }\n",
    "    }\n",
    "\n",
    "    instances = request[\"instances\"]\n",
    "    parameters = request[\"parameters\"]\n",
    "    k = parameters.get(\"k\", 10)\n",
    "\n",
    "    start_time = time.time()\n",
    "    predictions = [retrieval_predict(**instance, k = k) for instance in instances]\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return end_time - start_time\n",
    "\n",
    "check_prediction_time(n = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"movie_titles\": [\n",
      "      \"Winnie the Pooh and the Blustery Day (1968)\",\n",
      "      \"Alien (1979)\",\n",
      "      \"Deconstructing Harry (1997)\",\n",
      "      \"Patton (1970)\",\n",
      "      \"To Wong Foo, Thanks for Everything! Julie Newmar (1995)\",\n",
      "      \"Of Love and Shadows (1994)\",\n",
      "      \"Miracle on 34th Street (1994)\",\n",
      "      \"Mask, The (1994)\",\n",
      "      \"Bed of Roses (1996)\",\n",
      "      \"Fly Away Home (1996)\"\n",
      "    ],\n",
      "    \"movie_scores\": [\n",
      "      9.653619766235352,\n",
      "      7.183511734008789,\n",
      "      7.077798843383789,\n",
      "      6.192534446716309,\n",
      "      6.092672348022461,\n",
      "      5.85479736328125,\n",
      "      5.565959930419922,\n",
      "      5.52648401260376,\n",
      "      5.500728607177734,\n",
      "      5.386192321777344\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"movie_titles\": [\n",
      "      \"Two if by Sea (1996)\",\n",
      "      \"Mask, The (1994)\",\n",
      "      \"Malice (1993)\",\n",
      "      \"Bridges of Madison County, The (1995)\",\n",
      "      \"Smoke (1995)\",\n",
      "      \"Firm, The (1993)\",\n",
      "      \"Star Wars (1977)\",\n",
      "      \"Beauty and the Beast (1991)\",\n",
      "      \"Cats Don't Dance (1997)\",\n",
      "      \"Clear and Present Danger (1994)\"\n",
      "    ],\n",
      "    \"movie_scores\": [\n",
      "      7.910170078277588,\n",
      "      7.693970203399658,\n",
      "      7.138026714324951,\n",
      "      6.736512660980225,\n",
      "      6.736512660980225,\n",
      "      6.564693927764893,\n",
      "      6.489426612854004,\n",
      "      5.966603755950928,\n",
      "      5.5782976150512695,\n",
      "      5.205721855163574\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "request = {\n",
    "    \"instances\": [\n",
    "        {\n",
    "            \"user_id\": \"41\",\n",
    "            \"exclusions\": [\"Fargo (1996)\"],\n",
    "        },\n",
    "        {\n",
    "            \"user_id\": \"42\",\n",
    "            \"exclusions\": [],\n",
    "        }\n",
    "    ],\n",
    "    \"parameters\": {\n",
    "        \"k\": 10\n",
    "    }\n",
    "}\n",
    "\n",
    "instances = request[\"instances\"]\n",
    "parameters = request[\"parameters\"]\n",
    "k = parameters.get(\"k\", 10)\n",
    "\n",
    "predictions = [retrieval_predict(**instance, k = k) for instance in instances]\n",
    "\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom predict to return retrieval results + ratings\n",
    "\n",
    "NOTE: This doesn't guarantee that all the retrieved results have the highest rating. Top K retrieval is based on the user and movie embedding models. Ratings are determined after top k results are retrieved.\n",
    "\n",
    "The ratings are tuned to user ratings (1-5) but aren't normalized. They can be by setting a min/max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_and_rating_predict(user_id, k, exclusions = []):\n",
    "    user_ids = tf.constant([user_id])\n",
    "    \n",
    "    if len(exclusions) > 0:\n",
    "        exclusions = tf.constant([exclusions])\n",
    "        k_scores, k_predictions = scann_loaded.query_with_exclusions(user_ids, exclusions, k)\n",
    "    else:\n",
    "        k_scores, k_predictions = scann_loaded.call(user_ids, k)\n",
    "\n",
    "    k_predictions_json = k_predictions.numpy().tolist()\n",
    "    k_encoded_predictions_json = [[pred.decode('utf-8') for pred in pred_list] for pred_list in k_predictions_json]\n",
    "\n",
    "    user_embeddings = user_model_loaded(user_ids)\n",
    "    movie_embeddings = movie_model_loaded(k_predictions)\n",
    "    \n",
    "    # Handle case when less than k candidates can be returned\n",
    "    movie_embedding_size = tf.size(movie_embeddings)\n",
    "    valid_k = min(k, movie_embedding_size // embedding_dimension)\n",
    "\n",
    "    flattened_movie_embeddings = tf.reshape(movie_embeddings, (valid_k, embedding_dimension))\n",
    "    repeated_user_embeddings = tf.repeat(user_embeddings, repeats=valid_k, axis=0)\n",
    "\n",
    "    flattened_movie_ratings = rating_model_loaded(tf.concat([repeated_user_embeddings, flattened_movie_embeddings], axis=1))\n",
    "\n",
    "    movie_ratings = tf.reshape(flattened_movie_ratings, (1, valid_k))\n",
    "    movie_ratings_json = movie_ratings.numpy().tolist()\n",
    "\n",
    "    predictions = {\n",
    "        \"movies\": k_encoded_predictions_json[0],\n",
    "        \"ratings\": movie_ratings_json[0]\n",
    "    }\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"movies\": [\n",
      "      \"Groundhog Day (1993)\",\n",
      "      \"Local Hero (1983)\",\n",
      "      \"Apollo 13 (1995)\",\n",
      "      \"Empire Strikes Back, The (1980)\",\n",
      "      \"E.T. the Extra-Terrestrial (1982)\",\n",
      "      \"Silence of the Lambs, The (1991)\",\n",
      "      \"Blues Brothers, The (1980)\",\n",
      "      \"It's a Wonderful Life (1946)\",\n",
      "      \"Penny Serenade (1941)\",\n",
      "      \"American in Paris, An (1951)\"\n",
      "    ],\n",
      "    \"ratings\": [\n",
      "      4.262782096862793,\n",
      "      4.885346412658691,\n",
      "      4.478445053100586,\n",
      "      4.4249725341796875,\n",
      "      4.450826644897461,\n",
      "      4.708822250366211,\n",
      "      4.400209426879883,\n",
      "      4.602962493896484,\n",
      "      4.568150520324707,\n",
      "      4.511090278625488\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"movies\": [\n",
      "      \"Rent-a-Kid (1995)\",\n",
      "      \"Paper, The (1994)\",\n",
      "      \"Sneakers (1992)\",\n",
      "      \"Black Sheep (1996)\",\n",
      "      \"Pinocchio (1940)\",\n",
      "      \"She's the One (1996)\",\n",
      "      \"Santa Clause, The (1994)\",\n",
      "      \"North (1994)\",\n",
      "      \"Lion King, The (1994)\",\n",
      "      \"Substitute, The (1996)\"\n",
      "    ],\n",
      "    \"ratings\": [\n",
      "      3.9304721355438232,\n",
      "      4.000694751739502,\n",
      "      3.9465887546539307,\n",
      "      3.9660966396331787,\n",
      "      4.221636772155762,\n",
      "      3.7678780555725098,\n",
      "      3.9106078147888184,\n",
      "      3.877626895904541,\n",
      "      4.389237403869629,\n",
      "      3.5870211124420166\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "request = {\n",
    "    \"instances\": [\n",
    "        {\n",
    "            \"user_id\": \"41\",\n",
    "            \"exclusions\": [\"Fargo (1996)\"],\n",
    "        },\n",
    "        {\n",
    "            \"user_id\": \"42\",\n",
    "            \"exclusions\": [],\n",
    "        }\n",
    "    ],\n",
    "    \"parameters\": {\n",
    "        \"k\": 10\n",
    "    }\n",
    "}\n",
    "\n",
    "instances = request[\"instances\"]\n",
    "parameters = request[\"parameters\"]\n",
    "k = parameters.get(\"k\", 10)\n",
    "\n",
    "predictions = [retrieval_and_rating_predict(**instance, k = k) for instance in instances]\n",
    "\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "basic_retrieval.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-13.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-13:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
